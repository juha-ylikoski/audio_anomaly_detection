@article{balle2016,
  abstract  = {We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.},
  author    = {Ballé, Johannes and Valero Laparra and Simoncelli, Eero P},
  address   = {Ithaca},
  copyright = {2017. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  issn      = {2331-8422},
  journal   = {arXiv.org},
  keywords  = {Artificial neural networks ; Biological models (mathematics) ; Distortion ; Image compression ; Image quality ; JPEG encoders-decoders ; Linear filters ; Neural networks ; Nonlinear analysis ; Nonlinearity ; Transformations (mathematics) ; Visual observation},
  language  = {eng},
  publisher = {Cornell University Library, arXiv.org},
  title     = {End-to-end Optimized Image Compression},
  year      = {2017}
}


@article{balle2018,
  abstract  = {We describe an end-to-end trainable model for image compression based on variational autoencoders. The model incorporates a hyperprior to effectively capture spatial dependencies in the latent representation. This hyperprior relates to side information, a concept universal to virtually all modern image codecs, but largely unexplored in image compression using artificial neural networks (ANNs). Unlike existing autoencoder compression methods, our model trains a complex prior jointly with the underlying autoencoder. We demonstrate that this model leads to state-of-the-art image compression when measuring visual quality using the popular MS-SSIM index, and yields rate-distortion performance surpassing published ANN-based methods when evaluated using a more traditional metric based on squared error (PSNR). Furthermore, we provide a qualitative comparison of models trained for different distortion metrics.},
  author    = {Ballé, Johannes and Minnen, David and Singh, Saurabh and Sung Jin Hwang and Johnston, Nick},
  address   = {Ithaca},
  copyright = {2018. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  issn      = {2331-8422},
  journal   = {arXiv.org},
  keywords  = {Artificial neural networks ; Compression tests ; Distortion ; Image compression ; Image quality ; Neural networks ; Spatial dependencies},
  language  = {eng},
  publisher = {Cornell University Library, arXiv.org},
  title     = {Variational image compression with a scale hyperprior},
  year      = {2018}
}


@inproceedings{C2,
  author       = {Jones, C.D. and Smith, A.B. and Roberts, E.F.},
  title        = {Article Title},
  booktitle    = {Proceedings Title},
  organization = {IEEE},
  year         = {2003},
  volume       = {II},
  pages        = {803-806}
}

@article{checkerboard,
  abstract  = {For learned image compression, the autoregressive context model is proved effective in improving the rate-distortion (RD) performance. Because it helps remove spatial redundancies among latent representations. However, the decoding process must be done in a strict scan order, which breaks the parallelization. We propose a parallelizable checkerboard context model (CCM) to solve the problem. Our two-pass checkerboard context calculation eliminates such limitations on spatial locations by re-organizing the decoding order. Speeding up the decoding process more than 40 times in our experiments, it achieves significantly improved computational efficiency with almost the same rate-distortion performance. To the best of our knowledge, this is the first exploration on parallelization-friendly spatial context model for learned image compression.},
  author    = {He, Dailan and Zheng, Yaoyan and Sun, Baocheng and Wang, Yan and Qin, Hongwei},
  address   = {Ithaca},
  copyright = {2021. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  issn      = {2331-8422},
  journal   = {arXiv.org},
  keywords  = {Autoregressive models ; Context ; Distortion ; Image compression ; Parallel processing},
  language  = {eng},
  publisher = {Cornell University Library, arXiv.org},
  title     = {Checkerboard Context Model for Efficient Learned Image Compression},
  year      = {2021}
}
@article{compressai,
  title   = {CompressAI: a PyTorch library and evaluation platform for end-to-end compression research},
  author  = {B{\'e}gaint, Jean and Racap{\'e}, Fabien and Feltman, Simon and Pushparaja, Akshay},
  year    = {2020},
  journal = {arXiv preprint arXiv:2011.03029}
}

@article{diffusion,
  abstract  = {We consider a novel lossy compression approach based on unconditional diffusion generative models, which we call DiffC. Unlike modern compression schemes which rely on transform coding and quantization to restrict the transmitted information, DiffC relies on the efficient communication of pixels corrupted by Gaussian noise. We implement a proof of concept and find that it works surprisingly well despite the lack of an encoder transform, outperforming the state-of-the-art generative compression method HiFiC on ImageNet 64x64. DiffC only uses a single model to encode and denoise corrupted pixels at arbitrary bitrates. The approach further provides support for progressive coding, that is, decoding from partial bit streams. We perform a rate-distortion analysis to gain a deeper understanding of its performance, providing analytical results for multivariate Gaussian data as well as theoretic bounds for general distributions. Furthermore, we prove that a flow-based reconstruction achieves a 3 dB gain over ancestral sampling at high bitrates.},
  author    = {Theis, Lucas and Salimans, Tim and Hoffman, Matthew D and Mentzer, Fabian},
  address   = {Ithaca},
  copyright = {2022. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  issn      = {2331-8422},
  journal   = {arXiv.org},
  keywords  = {Coders ; Coding ; Decoding ; Multivariate analysis ; Pixels ; Random noise},
  language  = {eng},
  publisher = {Cornell University Library, arXiv.org},
  title     = {Lossy Compression with Gaussian Diffusion},
  year      = {2022}
}
@article{ELIC,
  abstract  = {Recently, learned image compression techniques have achieved remarkable performance, even surpassing the best manually designed lossy image coders. They are promising to be large-scale adopted. For the sake of practicality, a thorough investigation of the architecture design of learned image compression, regarding both compression performance and running speed, is essential. In this paper, we first propose uneven channel-conditional adaptive coding, motivated by the observation of energy compaction in learned image compression. Combining the proposed uneven grouping model with existing context models, we obtain a spatial-channel contextual adaptive model to improve the coding performance without damage to running speed. Then we study the structure of the main transform and propose an efficient model, ELIC, to achieve state-of-the-art speed and compression ability. With superior performance, the proposed model also supports extremely fast preview decoding and progressive decoding, which makes the coming application of learning-based image compression more promising.},
  author    = {He, Dailan and Yang, Ziming and Peng, Weikun and Ma, Rui and Qin, Hongwei and Wang, Yan},
  address   = {Ithaca},
  copyright = {2022. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  issn      = {2331-8422},
  journal   = {arXiv.org},
  keywords  = {Coders ; Decoding ; Image coding ; Image compression},
  language  = {eng},
  publisher = {Cornell University Library, arXiv.org},
  title     = {ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding},
  year      = {2022}
}
@article{HIFIC,
  abstract  = {We extensively study how to combine Generative Adversarial Networks and learned compression to obtain a state-of-the-art generative lossy compression system. In particular, we investigate normalization layers, generator and discriminator architectures, training strategies, as well as perceptual losses. In contrast to previous work, i) we obtain visually pleasing reconstructions that are perceptually similar to the input, ii) we operate in a broad range of bitrates, and iii) our approach can be applied to high-resolution images. We bridge the gap between rate-distortion-perception theory and practice by evaluating our approach both quantitatively with various perceptual metrics, and with a user study. The study shows that our method is preferred to previous approaches even if they use more than 2x the bitrate.},
  author    = {Mentzer, Fabian and Toderici, George and Tschannen, Michael and Agustsson, Eirikur},
  address   = {Ithaca},
  copyright = {2020. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  issn      = {2331-8422},
  journal   = {arXiv.org},
  keywords  = {Image compression ; Image resolution},
  language  = {eng},
  publisher = {Cornell University Library, arXiv.org},
  title     = {High-Fidelity Generative Image Compression},
  year      = {2020}
}


@inproceedings{imagenet,
  title        = {Imagenet: A large-scale hierarchical image database},
  author       = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle    = {2009 IEEE conference on computer vision and pattern recognition},
  pages        = {248--255},
  year         = {2009},
  organization = {Ieee}
}

@article{mbt2018,
  abstract  = {Recent models for learned image compression are based on autoencoders, learning approximately invertible mappings from pixels to a quantized latent representation. These are combined with an entropy model, a prior on the latent representation that can be used with standard arithmetic coding algorithms to yield a compressed bitstream. Recently, hierarchical entropy models have been introduced as a way to exploit more structure in the latents than simple fully factorized priors, improving compression performance while maintaining end-to-end optimization. Inspired by the success of autoregressive priors in probabilistic generative models, we examine autoregressive, hierarchical, as well as combined priors as alternatives, weighing their costs and benefits in the context of image compression. While it is well known that autoregressive models come with a significant computational penalty, we find that in terms of compression performance, autoregressive and hierarchical priors are complementary and, together, exploit the probabilistic structure in the latents better than all previous learned models. The combined model yields state-of-the-art rate--distortion performance, providing a 15.8% average reduction in file size over the previous state-of-the-art method based on deep learning, which corresponds to a 59.8% size reduction over JPEG, more than 35% reduction compared to WebP and JPEG2000, and bitstreams 8.4% smaller than BPG, the current state-of-the-art image codec. To the best of our knowledge, our model is the first learning-based method to outperform BPG on both PSNR and MS-SSIM distortion metrics.},
  author    = {Minnen, David and Ballé, Johannes and Toderici, George},
  address   = {Ithaca},
  copyright = {2018. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  issn      = {2331-8422},
  journal   = {arXiv.org},
  keywords  = {Algorithms ; Arithmetic coding ; Autoregressive models ; Codec ; Distortion ; Entropy ; Image compression ; JPEG encoders-decoders ; Machine learning ; Optimization ; Representations ; Size reduction},
  language  = {eng},
  publisher = {Cornell University Library, arXiv.org},
  title     = {Joint Autoregressive and Hierarchical Priors for Learned Image Compression},
  year      = {2018}
}

@inproceedings{PO-ELIC,
  abstract  = {In the past years, learned image compression (LIC) has achieved remarkable performance. The recent LIC methods outperform VVC in both PSNR and MS-SSIM. However, the low bit-rate reconstructions of LIC suffer from artifacts such as blurring, color drifting and texture missing. Moreover, those varied artifacts make image quality metrics correlate badly with human perceptual quality. In this paper, we propose PO-ELIC, i.e., Perception-Oriented Efficient Learned Image Coding. To be specific, we adapt ELIC, one of the state-of-the-art LIC models, with adversarial training techniques. We apply a mixture of losses including hinge-form adversarial loss, Charbonnier loss, and style loss, to finetune the model towards better perceptual quality. Experimental results demonstrate that our method achieves comparable perceptual quality with HiFiC with much lower bitrate.},
  author    = {He, Dailan and Yang, Ziming and Yu, Hongjiu and Xu, Tongda and Luo, Jixiang and Chen, Yuan and Gao, Chenjian and Shi, Xinjie and Qin, Hongwei and Wang, Yan},
  address   = {Piscataway},
  booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  copyright = {Copyright The Institute of Electrical and Electronics Engineers, Inc. (IEEE) 2022},
  isbn      = {1665487399},
  issn      = {2160-7516},
  keywords  = {Adaptation models ; Blurring ; Computer vision ; Image coding ; Image color analysis ; Image compression ; Image quality ; Measurement ; Pattern recognition ; Perception ; Training ; Visualization},
  language  = {eng},
  pages     = {1763-1768},
  publisher = {IEEE},
  title     = {PO-ELIC: Perception-Oriented Efficient Learned Image Coding},
  year      = {2022}
}



@article{LeeJooyoung2019CEMf,
abstract = {We propose a context-adaptive entropy model for use in end-to-end optimized image compression. Our model exploits two types of contexts, bit-consuming contexts and bit-free contexts, distinguished based upon whether additional bit allocation is required. Based on these contexts, we allow the model to more accurately estimate the distribution of each latent representation with a more generalized form of the approximation models, which accordingly leads to an enhanced compression performance. Based on the experimental results, the proposed method outperforms the traditional image codecs, such as BPG and JPEG2000, as well as other previous artificial-neural-network (ANN) based approaches, in terms of the peak signal-to-noise ratio (PSNR) and multi-scale structural similarity (MS-SSIM) index.},
author = {Lee, Jooyoung and Cho, Seunghyun and Seung-Kwon Beack},
address = {Ithaca},
copyright = {2019. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Entropy ; Image compression ; Neural networks},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
title = {Context-adaptive Entropy Model for End-to-end Optimized Image Compression},
year = {2019},
}



@article{AgustssonEirikur2017SVQf,
abstract = {We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both.},
author = {Agustsson, Eirikur and Mentzer, Fabian and Tschannen, Michael and Cavigelli, Lukas and Timofte, Radu and Benini, Luca and Luc Van Gool},
address = {Ithaca},
copyright = {2017. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Compressibility ; Image compression ; Neural networks ; Representations ; Training ; Vector quantization},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
title = {Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations},
year = {2017},
}


@inproceedings{VincentPascal2008Eacr,
abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
copyright = {Copyright 2020 Elsevier B.V., All rights reserved.},
isbn = {9781605582054},
language = {eng},
pages = {1096-1103},
publisher = {ACM},
series = {ICML '08},
title = {Extracting and composing robust features with denoising autoencoders},
year = {2008},
}


@inproceedings{TheisLucas2017Licw,
author = {Theis, Lucas and Shi, Wenzhe and Cunningham, Andrew and Huszár, Ferenc},
booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
copyright = {Copyright 2020 Elsevier B.V., All rights reserved.},
language = {eng},
title = {Lossy image compression with compressive autoencoders},
year = {2017},
}



@article{XueYuyang2019ABIC,
abstract = {The traditional image compressors, e.g., BPG and H.266, have achieved great image and video compression quality. Recently, Convolutional Neural Network has been used widely in image compression. We proposed an attention-based convolutional neural network for low bit-rate compression to post-process the output of traditional image compression decoder. Across the experimental results on validation sets, the post-processing module trained by MAE and MS-SSIM losses yields the highest PSNR of 32.10 on average at the bit-rate of 0.15.},
author = {Xue, Yuyang and Su, Jiannan},
address = {Ithaca},
copyright = {2019. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
issn = {2331-8422},
journal = {arXiv.org},
keywords = {Artificial neural networks ; Compressors ; Image compression ; Image quality ; Neural networks ; Post-processing ; Video compression},
language = {eng},
publisher = {Cornell University Library, arXiv.org},
title = {Attention Based Image Compression Post-Processing Convolutional Neural Network},
year = {2019},
}

@inproceedings{kodak,
author = {Eastman Kodak},
booktitle = {http://r0k.us/graphics/kodak},
copyright = {Copyright 1993, All rights reserved.},
language = {eng},
title = {Kodak lossless true color image suite (pho- tocd pcd0992)},
year = {1993},

}

@inproceedings{clic,

booktitle = {http://www.compression.cc},
copyright = {Copyright 2020, All rights reserved.},
language = {eng},
title = {Workshop and challenge on learned image compression (clic)},
year = {2020},
}

@inproceedings{WangZhou2003Mssf,
author = {Wang, Zhou and Simoncelli, Eero P. and Bovik, Alan C.},
booktitle = {Conference Record of the Asilomar Conference on Signals, Systems and Computers},
copyright = {Copyright 2012 Elsevier B.V., All rights reserved.},
issn = {1058-6393},
language = {eng},
pages = {1398-1402},
title = {Multi-scale structural similarity for image quality assessment},
volume = {2},
year = {2003},
}


@inproceedings{Bjntegaard2001CalculationOA,
  title={Calculation of Average PSNR Differences between RD-curves},
  author={Gisle Bj{\o}ntegaard},
  year={2001}
}
